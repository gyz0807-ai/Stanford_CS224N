# Stanford CS224N (2019 Winter)
Course website: http://web.stanford.edu/class/cs224n/

## Contents
1. Introduction and Word Vectors 
2. Word Vectors 2 and Word Senses
3. Word Window Classification, Neural Networks, and Matrix Calculus 
4. Backpropagation and Computation Graphs 
5. Linguistic Structure: Dependency Parsing 
6. The probability of a sentence? Recurrent Neural Networks and Language Models 
7. Vanishing Gradients and Fancy RNNs 
8. Machine Translation, Seq2Seq and Attention 
9. ConvNets for NLP 
10. Information from parts of words: Subword Models 
11. Modeling contexts of use: Contextual Representations and Pretraining 
12. Transformers and Self-Attention For Generative Models 
13. Natural Language Generation 
14. Reference in Language and Coreference Resolution 
15. Multitask Learning: A general model for NLP?
16. Constituency Parsing and Tree Recursive Neural Networks 
17. Safety, Bias, and Fairness
18. Future of NLP + Deep Learning 

## Assignments
Assignment 1: Introduction to word vectors  
Assignment 2: Derivatives and implementation of word2vec algorithm  
Assignment 3: Dependency parsing and neural network foundations    
Assignment 4: Neural Machine Translation with sequence-to-sequence and attention  
Assignment 5: Neural Machine Translation with ConvNets and subword modeling  
